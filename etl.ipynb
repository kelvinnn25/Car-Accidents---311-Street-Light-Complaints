{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install sodapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_8fDvdyj8Dy",
        "outputId": "a019950d-c08e-4b5d-a4a6-cda6c7d3177e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sodapy\n",
            "  Downloading sodapy-2.2.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests>=2.28.1 in /usr/local/lib/python3.10/dist-packages (from sodapy) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.1->sodapy) (2023.7.22)\n",
            "Installing collected packages: sodapy\n",
            "Successfully installed sodapy-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-cloud-bigquery"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaNBlzJff8gd",
        "outputId": "a5455e4e-ba8b-4a2b-f598-ad501f93e649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.12.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.47.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (1.59.2)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.11.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (1.22.3)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.3.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.6.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (23.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.31.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.61.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (2.17.3)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.48.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (0.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraction of the 311 Service Request Dataset"
      ],
      "metadata": {
        "id": "IfC_6l3hG-Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sodapy import Socrata\n",
        "\n",
        "data_url='data.cityofnewyork.us'    # The Host Name for the API endpoint (the https:// part will be added automatically)\n",
        "data_set='erm2-nwe9'    # The data set at the API endpoint (311 data in this case)\n",
        "app_token='fRtgN6KKZ5bEIkbEB12tbzK7T'   # The app token created in the prior steps\n",
        "client = Socrata(data_url,app_token)      # Create the client to point to the API endpoint\n",
        "# Set the timeout to 60 seconds\n",
        "client.timeout = 60"
      ],
      "metadata": {
        "id": "_cnfkwd2RS9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = client.get_metadata(data_set)\n",
        "[x['name'] for x in metadata['columns']]\n",
        "['Unique Key', 'Created Date', 'Closed Date', 'Agency', 'Agency Name', 'Complaint Type', 'Descriptor',\n",
        "'Location Type', 'Incident Zip', 'Incident Address', 'Street Name', 'Cross Street 1', 'Cross Street 2',\n",
        "'Intersection Street 1', 'Intersection Street 2', 'Address Type', 'City', 'Landmark', 'Facility Type',\n",
        "'Status', 'Due Date', 'Resolution Description', 'Resolution Action Updated Date', 'Community Board', 'BBL',\n",
        "'Borough', 'X Coordinate (State Plane)', 'Y Coordinate (State Plane)', 'Open Data Channel Type',\n",
        "'Park Facility Name', 'Park Borough', 'Vehicle Type', 'Taxi Company Borough', 'Taxi Pick Up Location',\n",
        "'Bridge Highway Name', 'Bridge Highway Direction', 'Road Ramp', 'Bridge Highway Segment', 'Latitude',\n",
        "'Longitude', 'Location', 'Zip Codes', 'Community Districts', 'Borough Boundaries', 'City Council Districts',\n",
        "'Police Precincts']"
      ],
      "metadata": {
        "id": "lIyYkrHrSPLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba950fa-6c3a-437b-8776-a0362d911c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Unique Key',\n",
              " 'Created Date',\n",
              " 'Closed Date',\n",
              " 'Agency',\n",
              " 'Agency Name',\n",
              " 'Complaint Type',\n",
              " 'Descriptor',\n",
              " 'Location Type',\n",
              " 'Incident Zip',\n",
              " 'Incident Address',\n",
              " 'Street Name',\n",
              " 'Cross Street 1',\n",
              " 'Cross Street 2',\n",
              " 'Intersection Street 1',\n",
              " 'Intersection Street 2',\n",
              " 'Address Type',\n",
              " 'City',\n",
              " 'Landmark',\n",
              " 'Facility Type',\n",
              " 'Status',\n",
              " 'Due Date',\n",
              " 'Resolution Description',\n",
              " 'Resolution Action Updated Date',\n",
              " 'Community Board',\n",
              " 'BBL',\n",
              " 'Borough',\n",
              " 'X Coordinate (State Plane)',\n",
              " 'Y Coordinate (State Plane)',\n",
              " 'Open Data Channel Type',\n",
              " 'Park Facility Name',\n",
              " 'Park Borough',\n",
              " 'Vehicle Type',\n",
              " 'Taxi Company Borough',\n",
              " 'Taxi Pick Up Location',\n",
              " 'Bridge Highway Name',\n",
              " 'Bridge Highway Direction',\n",
              " 'Road Ramp',\n",
              " 'Bridge Highway Segment',\n",
              " 'Latitude',\n",
              " 'Longitude',\n",
              " 'Location',\n",
              " 'Zip Codes',\n",
              " 'Community Districts',\n",
              " 'Borough Boundaries',\n",
              " 'City Council Districts',\n",
              " 'Police Precincts']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the first 2000 results returned as JSON object from the API where the range falls within the years\n",
        "where_clause = \"complaint_type = 'Traffic Signal Condition' AND created_date BETWEEN '2018-01-01' AND '2023-12-31'\"\n",
        "results = client.get(data_set, where=where_clause,limit=100000000)"
      ],
      "metadata": {
        "id": "hPAHevIoTZ52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list of dictionaries to a Pandas data frame\n",
        "df = pd.DataFrame.from_records(results)\n",
        "# Save the data frame to a CSV file\n",
        "df.to_csv(\"my_311_data.csv\")\n",
        "row = 1\n",
        "df.at[row, 'incident_zip'] = 11234"
      ],
      "metadata": {
        "id": "1wvCP40XYe1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['created_date'].sort_values() #just checking to see if all the dates are valid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD9MbKrqY4Fv",
        "outputId": "44d05876-98b7-4bc4-82ef-03b1582d7e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "225138    2018-01-01T00:27:00.000\n",
              "225137    2018-01-01T01:08:00.000\n",
              "225136    2018-01-01T01:12:00.000\n",
              "225135    2018-01-01T02:24:00.000\n",
              "225134    2018-01-01T03:23:00.000\n",
              "                   ...           \n",
              "4         2023-11-24T23:15:00.000\n",
              "3         2023-11-24T23:28:00.000\n",
              "2         2023-11-24T23:37:00.000\n",
              "1         2023-11-25T00:25:00.000\n",
              "0         2023-11-25T00:28:00.000\n",
              "Name: created_date, Length: 225139, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "df['unique_key'] = pd.to_numeric(df['unique_key'], errors='coerce')\n",
        "\n",
        "# Replace NaN values with a default integer value or any other appropriate handling\n",
        "default_value = 0  # Replace with your desired default value\n",
        "df['unique_key'].fillna(default_value, inplace=True)\n",
        "\n",
        "# Convert the column to integers\n",
        "df['unique_key'] = df['unique_key'].astype(int)\n",
        "\n",
        "\n",
        "df['incident_zip'] = pd.to_numeric(df['incident_zip'], errors='coerce')\n",
        "df['incident_zip'].fillna(default_value, inplace=True)\n",
        "df['incident_zip'] = df['incident_zip'].astype(int)\n",
        "\n",
        "\n",
        "df['x_coordinate_state_plane'] = pd.to_numeric(df['x_coordinate_state_plane'], errors='coerce')\n",
        "df['x_coordinate_state_plane'].fillna(default_value, inplace=True)\n",
        "df['x_coordinate_state_plane'] = df['x_coordinate_state_plane'].astype(int)\n",
        "\n",
        "df['y_coordinate_state_plane'] = pd.to_numeric(df['y_coordinate_state_plane'], errors='coerce')\n",
        "df['y_coordinate_state_plane'].fillna(default_value, inplace=True)\n",
        "df['y_coordinate_state_plane'] = df['y_coordinate_state_plane'].astype(int)\n",
        "\n",
        "df['bbl'] = pd.to_numeric(df['bbl'], errors='coerce')\n",
        "df['bbl'].fillna(default_value, inplace=True)\n",
        "df['bbl'] = df['bbl'].astype(int)\n"
      ],
      "metadata": {
        "id": "7bYZ09sdkbMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "uploading to BQ\n"
      ],
      "metadata": {
        "id": "CQLDQY7oS32M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#from collab to google cloud storage\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "bucket_name = 'your_bucket_name'  # Replace with your GCS bucket name\n",
        "file_name = 'your_file_name.csv'  # Replace with your desired file name\n",
        "\n",
        "# Save DataFrame to CSV\n",
        "df.to_csv(file_name, index=False)\n",
        "\n",
        "# Upload the CSV file to GCS\n",
        "client = storage.Client()\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_name)\n",
        "blob.upload_from_filename(file_name)\n",
        "\n",
        "# from google storage to bq\n",
        "from google.cloud import bigquery\n",
        "\n",
        "dataset_id = 'your_dataset_id'  # Replace with your BigQuery dataset ID\n",
        "table_id = 'your_table_id'  # Replace with your BigQuery table ID\n",
        "\n",
        "# Create BigQuery dataset if it doesn't exist\n",
        "client = bigquery.Client()\n",
        "dataset_ref = client.dataset(dataset_id)\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "client.create_dataset(dataset, exists_ok=True)\n",
        "\n",
        "# Create BigQuery table\n",
        "table_ref = dataset_ref.table(table_id)\n",
        "job_config = bigquery.LoadJobConfig(schema=schema)\n",
        "\n",
        "# Load data from GCS to BigQuery\n",
        "uri = f\"gs://{bucket_name}/{file_name}\"\n",
        "load_job = client.load_table_from_uri(uri, table_ref, job_config=job_config)\n",
        "load_job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f\"Loaded {load_job.output_rows} rows into {dataset_id}.{table_id}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ctOZ6zJdrhJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import pandas\n",
        "import pytz\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "table_id = 'cis4400project-403800.projectDatasets.311_service_requests_backup'\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    # Specify a (partial) schema. All columns are always written to the\n",
        "    # table. The schema is used to assist in data type definitions.\n",
        "    schema=[\n",
        "        # Specify the type of columns whose type cannot be auto-detected. For\n",
        "        # example the \"title\" column uses pandas dtype \"object\", so its\n",
        "        # data type is ambiguous.\n",
        "        bigquery.SchemaField(\"unique_key\", bigquery.enums.SqlTypeNames.INTEGER,mode=\"NULLABLE\"),\n",
        "        # Indexes are written if included in the schema by name.\n",
        "        bigquery.SchemaField(\"created_date\", bigquery.enums.SqlTypeNames.TIMESTAMP,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"agency\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"agency_name\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"complaint_type\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"descriptor\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"incident_zip\", bigquery.enums.SqlTypeNames.INTEGER,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"intersection_street_1\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"intersection_street_2\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"address_type\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"city\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"facility_type\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"status\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"community_board\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"borough\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"x_coordinate_state_plane\", bigquery.enums.SqlTypeNames.INTEGER,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"y_coordinate_state_plane\", bigquery.enums.SqlTypeNames.INTEGER,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"open_data_channel_type\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"park_facility_name\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"park_borough\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"latitude\", bigquery.enums.SqlTypeNames.FLOAT,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"longitude\", bigquery.enums.SqlTypeNames.FLOAT,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"location\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"closed_date\", bigquery.enums.SqlTypeNames.TIMESTAMP,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"resolution_description\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"resolution_action_updated_date\", bigquery.enums.SqlTypeNames.TIMESTAMP,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"incident_address\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"street_name\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"cross_street_1\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"cross_street_2\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"bbl\", bigquery.enums.SqlTypeNames.INTEGER,mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"due_date\", bigquery.enums.SqlTypeNames.STRING,mode=\"NULLABLE\")\n",
        "    ],\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
        "    # disposition it replaces the table with the loaded data.\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        ")\n",
        "\n",
        "job = client.load_table_from_dataframe(\n",
        "    df, table_id, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete.\n",
        "\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "uYu3fGZ-elK5",
        "outputId": "7c5766ef-6c34-4252-b8f8-125da609cd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ArrowTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8816b85fe0b6>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m job = client.load_table_from_dataframe(\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m )  # Make an API request.\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mload_table_from_dataframe\u001b[0;34m(self, dataframe, destination, num_retries, job_id, job_id_prefix, location, project, job_config, parquet_compression, timeout)\u001b[0m\n\u001b[1;32m   2703\u001b[0m                         \u001b[0mparquet_compression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparquet_compression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2705\u001b[0;31m                     _pandas_helpers.dataframe_to_parquet(\n\u001b[0m\u001b[1;32m   2706\u001b[0m                         \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m                         \u001b[0mnew_job_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdataframe_to_parquet\u001b[0;34m(dataframe, bq_schema, filepath, parquet_compression, parquet_use_compliant_nested_type)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0mbq_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_schema_fields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbq_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m     \u001b[0marrow_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_to_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m     pyarrow.parquet.write_table(\n\u001b[1;32m    721\u001b[0m         \u001b[0marrow_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdataframe_to_arrow\u001b[0;34m(dataframe, bq_schema)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0marrow_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         arrow_arrays.append(\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0mbq_to_arrow_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_column_or_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         )\n\u001b[1;32m    664\u001b[0m         \u001b[0marrow_fields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbq_to_arrow_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbq_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrow_arrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mbq_to_arrow_array\u001b[0;34m(series, bq_field)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfield_type_upper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_STRUCT_TYPES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrow_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrow_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Array.from_pandas\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowTypeError\u001b[0m: object of type <class 'str'> cannot be converted to int"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "AkFE_fNylVLl",
        "outputId": "04b04613-4fa5-4870-9f7e-72b342e1071e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  unique_key             created_date agency                   agency_name  \\\n",
              "0   59540294  2023-11-25T00:28:00.000    DOT  Department of Transportation   \n",
              "1   59534495  2023-11-25T00:25:00.000    DOT  Department of Transportation   \n",
              "2   59535482  2023-11-24T23:37:00.000    DOT  Department of Transportation   \n",
              "3   59540095  2023-11-24T23:28:00.000    DOT  Department of Transportation   \n",
              "4   59535486  2023-11-24T23:15:00.000    DOT  Department of Transportation   \n",
              "\n",
              "             complaint_type  descriptor incident_zip intersection_street_1  \\\n",
              "0  Traffic Signal Condition  Controller        11385        CENTRAL AVENUE   \n",
              "1  Traffic Signal Condition  Controller        11234          FLATBUSH AVE   \n",
              "2  Traffic Signal Condition  Controller        11234       FLATBUSH AVENUE   \n",
              "3  Traffic Signal Condition        Post        11234       FLATBUSH AVENUE   \n",
              "4  Traffic Signal Condition  Controller        11207   PENNSYLVANIA AVENUE   \n",
              "\n",
              "  intersection_street_2  address_type  ...  \\\n",
              "0              67 PLACE  INTERSECTION  ...   \n",
              "1             TOYS R US  INTERSECTION  ...   \n",
              "2              AVENUE V  INTERSECTION  ...   \n",
              "3              AVENUE V  INTERSECTION  ...   \n",
              "4        STANLEY AVENUE  INTERSECTION  ...   \n",
              "\n",
              "                                            location              closed_date  \\\n",
              "0  {'latitude': '40.703379417547566', 'longitude'...                      NaN   \n",
              "1                                                NaN                      NaN   \n",
              "2  {'latitude': '40.60820274118944', 'longitude':...  2023-11-24T23:55:00.000   \n",
              "3  {'latitude': '40.60820274118944', 'longitude':...  2023-11-24T23:55:00.000   \n",
              "4  {'latitude': '40.656979266383345', 'longitude'...                      NaN   \n",
              "\n",
              "                              resolution_description  \\\n",
              "0                                                NaN   \n",
              "1                                                NaN   \n",
              "2  Service Request status for this request is ava...   \n",
              "3  Service Request status for this request is ava...   \n",
              "4                                                NaN   \n",
              "\n",
              "  resolution_action_updated_date incident_address street_name cross_street_1  \\\n",
              "0                            NaN              NaN         NaN            NaN   \n",
              "1                            NaN              NaN         NaN            NaN   \n",
              "2        2023-11-24T23:55:00.000              NaN         NaN            NaN   \n",
              "3        2023-11-24T23:55:00.000              NaN         NaN            NaN   \n",
              "4                            NaN              NaN         NaN            NaN   \n",
              "\n",
              "  cross_street_2  bbl due_date  \n",
              "0            NaN  NaN      NaN  \n",
              "1            NaN  NaN      NaN  \n",
              "2            NaN  NaN      NaN  \n",
              "3            NaN  NaN      NaN  \n",
              "4            NaN  NaN      NaN  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0036c4dc-a3fd-44dc-836b-078dec8bccd5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>unique_key</th>\n",
              "      <th>created_date</th>\n",
              "      <th>agency</th>\n",
              "      <th>agency_name</th>\n",
              "      <th>complaint_type</th>\n",
              "      <th>descriptor</th>\n",
              "      <th>incident_zip</th>\n",
              "      <th>intersection_street_1</th>\n",
              "      <th>intersection_street_2</th>\n",
              "      <th>address_type</th>\n",
              "      <th>...</th>\n",
              "      <th>location</th>\n",
              "      <th>closed_date</th>\n",
              "      <th>resolution_description</th>\n",
              "      <th>resolution_action_updated_date</th>\n",
              "      <th>incident_address</th>\n",
              "      <th>street_name</th>\n",
              "      <th>cross_street_1</th>\n",
              "      <th>cross_street_2</th>\n",
              "      <th>bbl</th>\n",
              "      <th>due_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>59540294</td>\n",
              "      <td>2023-11-25T00:28:00.000</td>\n",
              "      <td>DOT</td>\n",
              "      <td>Department of Transportation</td>\n",
              "      <td>Traffic Signal Condition</td>\n",
              "      <td>Controller</td>\n",
              "      <td>11385</td>\n",
              "      <td>CENTRAL AVENUE</td>\n",
              "      <td>67 PLACE</td>\n",
              "      <td>INTERSECTION</td>\n",
              "      <td>...</td>\n",
              "      <td>{'latitude': '40.703379417547566', 'longitude'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>59534495</td>\n",
              "      <td>2023-11-25T00:25:00.000</td>\n",
              "      <td>DOT</td>\n",
              "      <td>Department of Transportation</td>\n",
              "      <td>Traffic Signal Condition</td>\n",
              "      <td>Controller</td>\n",
              "      <td>11234</td>\n",
              "      <td>FLATBUSH AVE</td>\n",
              "      <td>TOYS R US</td>\n",
              "      <td>INTERSECTION</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59535482</td>\n",
              "      <td>2023-11-24T23:37:00.000</td>\n",
              "      <td>DOT</td>\n",
              "      <td>Department of Transportation</td>\n",
              "      <td>Traffic Signal Condition</td>\n",
              "      <td>Controller</td>\n",
              "      <td>11234</td>\n",
              "      <td>FLATBUSH AVENUE</td>\n",
              "      <td>AVENUE V</td>\n",
              "      <td>INTERSECTION</td>\n",
              "      <td>...</td>\n",
              "      <td>{'latitude': '40.60820274118944', 'longitude':...</td>\n",
              "      <td>2023-11-24T23:55:00.000</td>\n",
              "      <td>Service Request status for this request is ava...</td>\n",
              "      <td>2023-11-24T23:55:00.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59540095</td>\n",
              "      <td>2023-11-24T23:28:00.000</td>\n",
              "      <td>DOT</td>\n",
              "      <td>Department of Transportation</td>\n",
              "      <td>Traffic Signal Condition</td>\n",
              "      <td>Post</td>\n",
              "      <td>11234</td>\n",
              "      <td>FLATBUSH AVENUE</td>\n",
              "      <td>AVENUE V</td>\n",
              "      <td>INTERSECTION</td>\n",
              "      <td>...</td>\n",
              "      <td>{'latitude': '40.60820274118944', 'longitude':...</td>\n",
              "      <td>2023-11-24T23:55:00.000</td>\n",
              "      <td>Service Request status for this request is ava...</td>\n",
              "      <td>2023-11-24T23:55:00.000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59535486</td>\n",
              "      <td>2023-11-24T23:15:00.000</td>\n",
              "      <td>DOT</td>\n",
              "      <td>Department of Transportation</td>\n",
              "      <td>Traffic Signal Condition</td>\n",
              "      <td>Controller</td>\n",
              "      <td>11207</td>\n",
              "      <td>PENNSYLVANIA AVENUE</td>\n",
              "      <td>STANLEY AVENUE</td>\n",
              "      <td>INTERSECTION</td>\n",
              "      <td>...</td>\n",
              "      <td>{'latitude': '40.656979266383345', 'longitude'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0036c4dc-a3fd-44dc-836b-078dec8bccd5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0036c4dc-a3fd-44dc-836b-078dec8bccd5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0036c4dc-a3fd-44dc-836b-078dec8bccd5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c2f6b0de-d73f-4a06-b821-922f09dd8130\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c2f6b0de-d73f-4a06-b821-922f09dd8130')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c2f6b0de-d73f-4a06-b821-922f09dd8130 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractions For Collisions dataset"
      ],
      "metadata": {
        "id": "Ynh_E_kXYU-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url2='data.cityofnewyork.us'    # The Host Name for the API endpoint (the https:// part will be added automatically)\n",
        "data_set2='h9gi-nx95'    # The data set at the API endpoint (311 data in this case)\n",
        "app_token2='fRtgN6KKZ5bEIkbEB12tbzK7T'   # The app token created in the prior steps\n",
        "client = Socrata(data_url2,app_token2)      # Create the client to point to the API endpoint\n",
        "# Set the timeout to 60 seconds\n",
        "client.timeout = 60\n",
        "# Retrieve the first 2000 results returned as JSON object from the API\n",
        "# The SoDaPy library converts this JSON object to a Python list of dictionaries\n",
        "where_clause2 = \"date_extract_y(crash_date) BETWEEN 2018 AND 2023\"\n",
        "results2 = client.get(data_set2, where=where_clause2, limit=100000000)"
      ],
      "metadata": {
        "id": "G6dOVaPbUHoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list of dictionaries to a Pandas data frame\n",
        "df2 = pd.DataFrame.from_records(results2)\n",
        "# Save the data frame to a CSV file\n",
        "df2.to_csv(\"collisions.csv\")"
      ],
      "metadata": {
        "id": "EtnL-IOEm-OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2['crash_date'].sort_values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcONX6rzl2Q1",
        "outputId": "5fec7fb9-0235-41e2-97ee-9d2e74230274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "730557    2018-01-01T00:00:00.000\n",
              "730049    2018-01-01T00:00:00.000\n",
              "730064    2018-01-01T00:00:00.000\n",
              "730069    2018-01-01T00:00:00.000\n",
              "724687    2018-01-01T00:00:00.000\n",
              "                   ...           \n",
              "843680    2023-11-21T00:00:00.000\n",
              "843681    2023-11-21T00:00:00.000\n",
              "843682    2023-11-21T00:00:00.000\n",
              "843669    2023-11-21T00:00:00.000\n",
              "843301    2023-11-21T00:00:00.000\n",
              "Name: crash_date, Length: 856052, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading COLLISIONS to BQ\n"
      ],
      "metadata": {
        "id": "SbvP-nZMSzBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import pandas\n",
        "import pytz\n",
        "\n",
        "# Construct a BigQuery client object.\n",
        "client = bigquery.Client()\n",
        "\n",
        "table_id = 'cis4400project-403800.projectDatasets.vehicle_collisions'\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    # Specify a (partial) schema. All columns are always written to the\n",
        "    # table. The schema is used to assist in data type definitions.\n",
        "    schema=[\n",
        "        # Specify the type of columns whose type cannot be auto-detected. For\n",
        "        # example the \"title\" column uses pandas dtype \"object\", so its\n",
        "        # data type is ambiguous.\n",
        "        # Indexes are written if included in the schema by name.\n",
        "        bigquery.SchemaField(\"crash_date\", bigquery.enums.SqlTypeNames.TIMESTAMP),\n",
        "        bigquery.SchemaField(\"crash_time\", bigquery.enums.SqlTypeNames.TIMESTAMP),\n",
        "        bigquery.SchemaField(\"on_street_name\", bigquery.enums.SqlTypeNames.STRING, mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\"),\n",
        "        bigquery.SchemaField(\"\", bigquery.enums.SqlTypeNames., mode=\"NULLABLE\")\n",
        "\n",
        "    ],\n",
        "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
        "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
        "    # disposition it replaces the table with the loaded data.\n",
        "    write_disposition=\"WRITE_TRUNCATE\",\n",
        ")\n",
        "\n",
        "job = client.load_table_from_dataframe(\n",
        "    df, table_id, job_config=job_config\n",
        ")  # Make an API request.\n",
        "job.result()  # Wait for the job to complete.\n",
        "\n",
        "table = client.get_table(table_id)  # Make an API request.\n",
        "print(\n",
        "    \"Loaded {} rows and {} columns to {}\".format(\n",
        "        table.num_rows, len(table.schema), table_id\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Lm_ewecNi1HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fact Tables ETL\n",
        "\n",
        "\n",
        "Notes:\n",
        "\n",
        "Have not defined the path yet"
      ],
      "metadata": {
        "id": "gAD-5d65hqdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ETL Complaint Facts\n",
        "# If using the native Google BigQuery API module:\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "# import credentials\n",
        "import pandas as pd\n",
        "import os\n",
        "import pyarrow\n",
        "from datetime import datetime\n",
        "from google.oauth2 import service_account"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlAUzdOtiiJ1",
        "outputId": "02939aa9-ecf5-41a5-f050-73db5ae5aa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame\n",
        "# Set the name of the dimension\n",
        "fact_name = 'complaints'\n",
        "\n",
        "# Set the GCP Project, dataset and table name\n",
        "gcp_project = 'cis4400project-403800'\n",
        "bq_dataset = '311_service_requests'\n",
        "table_name = fact_name + '_fact'\n",
        "# Construct the full BigQuery path to the table\n",
        "fact_table_path = \".\".join([gcp_project,bq_dataset,table_name])\n",
        "\n",
        "# Set the path to the source data files\n",
        "# For Linux use something like    /home/username/python_etl\n",
        "# For Mac use something like     /users/username/python_etl\n",
        "# file_source_path = 'c:\\\\Python_ETL'\n",
        "file_source_path = '/content' #what do we want to define here\n",
        "path_to_service_account_key_file = '/content/drive/MyDrive/cis4400project-403800-81bc1b0a588c(1).json'\n",
        "def transform_data( df):\n",
        "    \"\"\"\n",
        "    transform_data\n",
        "    Accepts a data frame\n",
        "    Performs any specific cleaning and transformation steps on the dataframe\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    # Convert the date_of_birth to a datetime64 data type. 2012-08-21 04:12:16.827\n",
        "    df['date_of_birth'] = pd.to_datetime(df['date_of_birth'], format='%m/%d/%Y')\n",
        "    # Convert the postal code into a string\n",
        "    df['incident_zip'] =  df['incident_zip'].astype(str)\n",
        "    return df\n",
        "def upload_bigquery_table(bqclient, table_path, write_disposition, df):\n",
        "    \"\"\"\n",
        "    upload_bigquery_table\n",
        "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
        "    Loads the data into the BigQuery table from the dataframe.\n",
        "    for credentials.\n",
        "    The write disposition is either\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
        "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
        "\n",
        "        # Submit the job\n",
        "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
        "\n",
        "        # Show the job results\n",
        "        job.result()\n",
        "    except Exception as err:\n",
        "        print(\"Failed to load BigQuery Table.\", err)\n",
        "        # os._exit(-1)\n",
        "\n",
        "\n",
        "def bigquery_table_exists(table_path, bqclient):\n",
        "    \"\"\"\n",
        "    bigquery_table_exists\n",
        "    Accepts a path to a BigQuery table\n",
        "    Checks if the BigQuery table exists.\n",
        "    Returns True or False\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bqclient.get_table(table_path)  # Make an API request.\n",
        "        return True\n",
        "    except NotFound:\n",
        "        # print(\"Table {} is not found.\".format(table_id))\n",
        "        return False\n",
        "def query_bigquery_table(table_path, bqclient, surrogate_key):\n",
        "    \"\"\"\n",
        "    query_bigquery_table\n",
        "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
        "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
        "    Returns the dataframe\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
        "    try:\n",
        "        bq_df = bqclient.query(sql_query).to_dataframe()\n",
        "    except Exception as err:\n",
        "        print(\"error\")\n",
        "    return bq_df\n",
        "\n",
        "def dimension_lookup( dimension_name='agency', lookup_columns=['agency', 'agency_name'], df=df):\n",
        "    \"\"\"\n",
        "    dimension_lookup\n",
        "    Lookup the lookup_columns in the dimension_name and return the associated surrogate keys\n",
        "    Returns dataframe augmented with the surrogate keys\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    surrogate_key = dimension_name+\"_dim_id\"\n",
        "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
        "    # Fetch the existing table\n",
        "    bq_df = query_bigquery_table( dimension_table_path, bqclient, surrogate_key)\n",
        "    #print(bq_df)\n",
        "    # Melt the dimension dataframe into an index with the lookup columns\n",
        "    m = bq_df.melt(id_vars=lookup_columns, value_vars=surrogate_key)\n",
        "    #print(m)\n",
        "    # Rename the \"value\" column to the surrogate key column name\n",
        "    m=m.rename(columns={\"value\":surrogate_key})\n",
        "    # Merge with the fact table record\n",
        "    df = df.merge(m, on=lookup_columns, how='left')\n",
        "    # Drop the \"variable\" column and the lookup columns\n",
        "    df = df.drop(columns=lookup_columns)\n",
        "    df = df.drop(columns=\"variable\")\n",
        "    #print(df)\n",
        "    return df\n",
        "\n",
        "def date_dimension_lookup(dimension_name='date', lookup_column='created_date', df=df):\n",
        "    \"\"\"\n",
        "    date_dimension_lookup\n",
        "    Lookup the lookup_columns in a date dimension and return the associated surrogate keys\n",
        "    Returns dataframe augmented with the surrogate keys\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    surrogate_key = dimension_name+\"_dim_id\"\n",
        "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
        "    # Fetch the existing table\n",
        "    bq_df = query_bigquery_table( dimension_table_path, bqclient, surrogate_key)\n",
        "    bq_df[\"full_date\"] = pd.to_datetime(bq_df.full_date, format=\"%Y-%m-%d %H:%M:%S\")\n",
        "    # Return just the date portion\n",
        "    bq_df[\"full_date\"] = bq_df.full_date.dt.date\n",
        "\n",
        "    # Dates in the 311 data look like this: 2017-08-11T11:57:00.000\n",
        "    # Extract the date from 'created_date' column\n",
        "    df[lookup_column] = pd.to_datetime(df[lookup_column], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
        "    # Return just the date portion\n",
        "    df[lookup_column] = df[lookup_column].dt.date\n",
        "\n",
        "    # Melt the dimension dataframe into an index with the lookup columns\n",
        "    m = bq_df.melt(id_vars='full_date', value_vars=surrogate_key)\n",
        "    # Rename the \"value\" column to the surrogate key column name\n",
        "    m=m.rename(columns={\"value\":lookup_column+\"_dim_id\"})\n",
        "\n",
        "    # Merge with the fact table record on the created_date\n",
        "    df = df.merge(m, left_on=lookup_column, right_on='full_date', how='left')\n",
        "\n",
        "    # Drop the \"variable\" column and the lookup columns\n",
        "    df = df.drop(columns=lookup_column)\n",
        "    df = df.drop(columns=\"variable\")\n",
        "    df = df.drop(columns=\"full_date\")\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def time_dimension_lookup( dimension_name='time', lookup_column='created_date', df=df):\n",
        "    \"\"\"\n",
        "    time_dimension_lookup\n",
        "    Lookup the lookup_columns in the time dimension and return the associated surrogate key\n",
        "    Returns dataframe augmented with the surrogate keys\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    surrogate_key = dimension_name+\"_dim_id\"\n",
        "    dimension_table_path = \".\".join([gcp_project,bq_dataset,dimension_name+\"_dimension\"])\n",
        "\n",
        "    # Dates in the 311 data look like this: 2017-08-11T11:57:00.000\n",
        "    # We can strip off the time portion after the letter \"T\" to ge the hours and minutes\n",
        "    # time_dim_id = (hours*60)+minutes+1\n",
        "    # Example:  Time is 22:07  so  (22*60)+7+1 = 1328\n",
        "    # Extract the date from 'created_date' column and save it in a temporary column\n",
        "    df[lookup_column+\"_newdate\"] = pd.to_datetime(df[lookup_column], format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
        "    # Strip off the hours and minutes portions\n",
        "    df[lookup_column+\"_hours\"] = df[lookup_column+\"_newdate\"].dt.strftime(\"%H\").astype(int)\n",
        "    # df[lookup_column+\"_hours\"] = df[lookup_column+\"_hours\"].astype(int)\n",
        "    df[lookup_column+\"_minutes\"] = df[lookup_column+\"_newdate\"].dt.strftime(\"%M\").astype(int)\n",
        "    # df[lookup_column+\"_minutes\"] = df[lookup_column+\"_minutes\"].astype(int)\n",
        "    # Now assign the time_dim_id\n",
        "    df[surrogate_key] = (df[lookup_column+\"_hours\"]*60)+df[lookup_column+\"_minutes\"]+1\n",
        "    print(\"Surrogate key is: \", surrogate_key)\n",
        "    print(df[surrogate_key])\n",
        "    # Drop the lookup time columns\n",
        "    df = df.drop(columns=lookup_column+\"_newdate\")\n",
        "    df = df.drop(columns=lookup_column+\"_hours\")\n",
        "    df = df.drop(columns=lookup_column+\"_minutes\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "AUy2wKr3i2pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    df = pd.DataFrame\n",
        "    # Create the BigQuery Client\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
        "\n",
        "    # Construct a BigQuery client object\n",
        "    bqclient = bigquery.Client()\n",
        "\n",
        "    # Load in the data file\n",
        "    # Load in the data file\n",
        "    with open(file_source_path, 'r') as data:\n",
        "            df = pd.read_csv(data)\n",
        "        # Set all of the column names to lower case letters\n",
        "    #print(df.head())\n",
        "    df = df.rename(columns=str.lower)    # If city is empty, fill it in with NEW YORK\n",
        "    df.city = df.city.fillna('NEW YORK')\n",
        "\n",
        "    # Consider removing columns that we will never use  df.drop([....])\n",
        "\n",
        "    # Lookup the agency dimension record  agency_dim_id\n",
        "    df = dimension_lookup( dimension_name='agency', lookup_columns=['agency', 'agency_name'], df=df)\n",
        "\n",
        "    # Lookup the location dimension record  location_dim_id\n",
        "    #df = dimension_lookup( dimension_name='location', lookup_columns=['borough', 'city', 'incident_address', 'incident_zip', 'latitude', 'longitude'], df=df)\n",
        "\n",
        "    # Lookup the channel  dimension record  channel_dim_id\n",
        "    #df = dimension_lookup( dimension_name='channel', lookup_columns=['open_data_channel_type', 'status'], df=df)\n",
        "\n",
        "    # Lookup the complaint_type  dimension record  complaint_type_dim_id\n",
        "    #df = dimension_lookup dimension_name='complaint_type', lookup_columns=['complaint_type', 'descriptor'], df=df)\n",
        "\n",
        "    # Lookup the time dimension record using the time part of the created_date\n",
        "    # Note - do this before looking up the date dimension\n",
        "    #df = time_dimension_lookup( dimension_name='time', lookup_column='created_date', df=df)\n",
        "    # The time_dimension_lookup returns a column named 'time_dim_id'. Rename this to the 'created_time_dim_id'\n",
        "    #df = df.rename(columns={'time_dim_id' : 'created_time_dim_id'})\n",
        "\n",
        "    # Lookup the created_date dimension record\n",
        "   # df = date_dimension_lookup(dimension_name='date', lookup_column='created_date', df=df)\n",
        "\n",
        "    # Lookup the closed_date dimension record\n",
        "    #df = date_dimension_lookup(dimension_name='date', lookup_column='closed_date', df=df)\n",
        "\n",
        "    # A list of all of the surrogate keys\n",
        "    # For transaction grain, also include the 'unique_key' column\n",
        "    surrogate_keys=['agency_dim_id','complaint_type_dim_id','channel_dim_id','location_dim_id','created_date_dim_id','created_time_dim_id','closed_date_dim_id']\n",
        "\n",
        "    # Remove all of the other non-surrogate key columns\n",
        "    df = df[surrogate_keys]\n",
        "\n",
        "    # For daily snapshot grain we:\n",
        "    # 1) Add a 'complaint_count' fact\n",
        "    # 2) Use Group By to count up the number of complaints, per location, per agency, etc. per day\n",
        "    # For transaction grain add in the unique_key but skip the above two steps.\n",
        "\n",
        "    # Add a complaint count (for daily snapshot grain)\n",
        "    df['complaint_count'] = 1\n",
        "    # Count up the number of complaints per agency, per location, etc. per day\n",
        "    #df = df.groupby(surrogate_keys)['complaint_count'].agg('count').reset_index()\n",
        "\n",
        "    # See if the target table exists\n",
        "    target_table_exists = bigquery_table_exists(fact_table_path, bqclient )\n",
        "    # If the target table does not exist, load all of the data into a new table\n",
        "    if not target_table_exists:\n",
        "        build_new_table( bqclient, fact_table_path, df)\n",
        "    # If the target table exists, then perform an incremental load\n",
        "    if target_table_exists:\n",
        "        insert_existing_table( bqclient, fact_table_path, df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "81HjhE3Gi8vH",
        "outputId": "57083b2f-224f-4e70-f651-a4fb2c2a7815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IsADirectoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-aad7cdeccf3f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load in the data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Load in the data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_source_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Set all of the column names to lower case letters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/content'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "311 Dimensions"
      ],
      "metadata": {
        "id": "anx-n0efnlTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If using the native Google BigQuery API module:\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "import pandas as pd\n",
        "import os\n",
        "import pyarrow\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import credentials\n",
        "from google.oauth2 import service_account\n",
        "\n",
        "# If using a service account key file, save the path to that file in credentials.py and import credentials\n",
        "path_to_service_account_key_file = \"/content/cis4400project-403800-81bc1b0a588c(1).json\"\n",
        "#!pip install credentials\n",
        "\n",
        "# Set the name of the dimension\n",
        "dimension_name = 'agency'\n",
        "\n",
        "# Set the name of the surrogate key\n",
        "surrogate_key = f\"{dimension_name}_dim_id\"\n",
        "\n",
        "# Set the name of the business key\n",
        "business_key = f'{dimension_name}_id'\n",
        "\n",
        "# Set the GCP Project, dataset and table name\n",
        "gcp_project = 'cis4400project-403800 '\n",
        "bq_dataset = '311_service_requests'\n",
        "table_name = f\"{dimension_name}_dimension\"\n",
        "# Construct the full BigQuery path to the table\n",
        "dimension_table_path = f\"{gcp_project}.{bq_dataset}.{table_name}\"\n",
        "\n",
        "# Set the path to the source data files. Use double-slash for Windows paths C:\\\\myfolder\n",
        "# For Linux use forward slashes    /home/username/python_etl\n",
        "# For Mac use forward slashes      /users/username/python_etl\n",
        "# file_source_path = 'c:\\\\Python_ETL'\n",
        "# file_source_path = 'C:\\\\Users\\\\rholo\\\\OneDrive\\\\Documents\\\\classes\\\\4400\\\\311'\n",
        "file_source_path = '/Contnet' #also have not defined this yet\n",
        "\n",
        "def transform_data(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    transform_data\n",
        "    Accepts a data frame\n",
        "    Performs any specific cleaning and transformation steps on the dataframe\n",
        "    Returns the modified dataframe\n",
        "    This function can be modified based on required changes\n",
        "    \"\"\"\n",
        "    # Select the columns for this dimension\n",
        "    column_list = ['agency','agency_name']\n",
        "    df = df[column_list]\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    return df\n",
        "\n",
        "def create_bigquery_client():\n",
        "    \"\"\"\n",
        "    create_bigquery_client\n",
        "    Creates a BigQuery client using the path to the service account key file\n",
        "    for credentials.\n",
        "    Returns the BigQuery client object\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # If authenticating using a service account key file, use the following code:\n",
        "        # bqclient = bigquery.Client.from_service_account_json(credentials.path_to_service_account_key_file)\n",
        "        # Google Colab authentication already completed\n",
        "        bqclient = bigquery.Client(gcp_project)\n",
        "        return bqclient\n",
        "    except Exception as err:\n",
        "        print(\"error\")\n",
        "        # os._exit(-1)\n",
        "    return bqclient\n",
        "\n",
        "def upload_bigquery_table(bqclient, table_path, write_disposition, df):\n",
        "    \"\"\"\n",
        "    upload_bigquery_table\n",
        "    Accepts a path to a BigQuery table, the write disposition and a dataframe\n",
        "    Loads the data into the BigQuery table from the dataframe.\n",
        "    for credentials.\n",
        "    The write disposition is either\n",
        "    write_disposition=\"WRITE_TRUNCATE\"  Erase the target data and load all new data.\n",
        "    write_disposition=\"WRITE_APPEND\"    Append to the existing table\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Set up a BigQuery job configuration with the write_disposition.\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=write_disposition)\n",
        "\n",
        "        # Submit the job\n",
        "        print(type(bqclient))\n",
        "        job = bqclient.load_table_from_dataframe(df, table_path, job_config=job_config)\n",
        "        # Show the job results\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        #os._exit(-1)\n",
        "\n",
        "def bigquery_table_exists(bqclient, table_path):\n",
        "    \"\"\"\n",
        "    bigquery_table_exists\n",
        "    Accepts a path to a BigQuery table\n",
        "    Checks if the BigQuery table exists.\n",
        "    Returns True or False\n",
        "    \"\"\"\n",
        "    try:\n",
        "        bqclient.get_table(table_path)  # Make an API request.\n",
        "        return True\n",
        "    except NotFound:\n",
        "        return False\n",
        "\n",
        "def query_bigquery_table(table_path, bqclient, surrogate_key):\n",
        "    \"\"\"\n",
        "    query_bigquery_table\n",
        "    Accepts a path to a BigQuery table and the name of the surrogate key\n",
        "    Queries the BigQuery table but leaves out the update_timestamp and surrogate key columns\n",
        "    Returns the dataframe\n",
        "    \"\"\"\n",
        "    bq_df = pd.DataFrame\n",
        "    sql_query = 'SELECT * EXCEPT ( update_timestamp, '+surrogate_key+') FROM `' + table_path + '`'\n",
        "    try:\n",
        "        bq_df = bqclient.query(sql_query).to_dataframe()\n",
        "    except Exception as err:\n",
        "        print(\"error\")\n",
        "    return bq_df\n",
        "\n",
        "def add_surrogate_key(df, dimension_name='customers', offset=1):\n",
        "    \"\"\"\n",
        "    add_surrogate_key\n",
        "    Accepts a data frame and inserts an integer identifier as the first column\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    # Reset the index to count from 0\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    # Add the new surrogate key starting from offset\n",
        "    df.insert(0, dimension_name+'_dim_id', df.index+offset)\n",
        "    return df\n",
        "\n",
        "def build_new_table(bqclient, dimension_table_path, dimension_name, df):\n",
        "    \"\"\"\n",
        "    build_new_table\n",
        "    Accepts a path to a dimensional table, the dimension name and a data frame\n",
        "    Add the surrogate key and a record timestamp to the data frame\n",
        "    Inserts the contents of the dataframe to the dimensional table.\n",
        "    \"\"\"\n",
        "    # Add a surrogate key\n",
        "    df = add_surrogate_key(df, dimension_name, 1)\n",
        "    # Add the update timestamp\n",
        "    # Upload the dataframe to the BigQuery table\n",
        "    upload_bigquery_table(bqclient, dimension_table_path, \"WRITE_TRUNCATE\", df)\n",
        "\n",
        "# Program main\n",
        "# Load the CSV File into a dataframe\n",
        "# Transform the Dataframe\n",
        "# Create a BigQuery client\n",
        "# See if the target dimension table exists\n",
        "#    If not exists, load the data into a new table\n",
        "#    If exists, insert new records into the table\n",
        "if __name__ == \"__main__\":\n",
        "    df = pd.DataFrame\n",
        "    # Load in the data file\n",
        "    with open(file_source_path, 'r') as data:\n",
        "            df = pd.read_csv(data)\n",
        "        # Set all of the column names to lower case letters\n",
        "    df = df.rename(columns=str.lower)\n",
        "\n",
        "\n",
        "    #df = load_csv_data_file(file_source_path, \"my_311_data_WaterQuality.csv\", df)\n",
        "    # Transform the data\n",
        "    df = transform_data(df)\n",
        "    # Create the BigQuery Client\n",
        "    # setup enviroment parameters to connect to BQ project\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path_to_service_account_key_file\n",
        "\n",
        "    # Construct a BigQuery client object\n",
        "    bqclient = bigquery.Client()\n",
        "\n",
        "    # See if the target dimensional table exists\n",
        "    target_table_exists = bigquery_table_exists(bqclient, dimension_table_path  )\n",
        "\n",
        "    # If the target dimension table does not exist, load all of the data into a new table\n",
        "    if not target_table_exists:\n",
        "        build_new_table( bqclient, dimension_table_path, dimension_name, df)\n",
        "    # If the target table exists, then perform an incremental load\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9NalTagVjhPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}